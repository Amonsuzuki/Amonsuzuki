{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amonsuzuki/Amonsuzuki/blob/main/20B_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut9wwIIjLjSv"
      },
      "source": [
        "**1. GPT-OSS 20Bのダウンロード**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607,
          "referenced_widgets": [
            "e4320887d6ba459ab06a07ae9edda2ed",
            "3cd1606c07cd4a37b08dd3ece50f5a2e",
            "d23af8ea061748968bca473d97c35102",
            "f989789ff25f4c18aa1ea197f5f765ab",
            "283f2ea51581440aa5bd364c78d853ca",
            "d3c19d3c5e434264aef0fa2c0ab1c647",
            "fa787ef1d3b84789b72e0490cacdaa33",
            "89d677b60099477592f23ce0c0b11cc9",
            "b72179316d204260a3679bdabc6dfece",
            "a8d6ebdb855a43f6b232135ec38e2514",
            "0e68ae72541248c4a661276ede542750"
          ]
        },
        "id": "_M0G-cWWzNq3",
        "outputId": "49055bf2-4c41-48e9-a2b1-840e19d85aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4320887d6ba459ab06a07ae9edda2ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GptOssForCausalLM(\n",
              "  (model): GptOssModel(\n",
              "    (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x GptOssDecoderLayer(\n",
              "        (self_attn): GptOssAttention(\n",
              "          (q_proj): Linear(in_features=2880, out_features=4096, bias=True)\n",
              "          (k_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
              "          (v_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
              "          (o_proj): Linear(in_features=4096, out_features=2880, bias=True)\n",
              "        )\n",
              "        (mlp): GptOssMLP(\n",
              "          (router): GptOssTopKRouter()\n",
              "          (experts): GptOssExperts()\n",
              "        )\n",
              "        (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
              "        (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
              "    (rotary_emb): GptOssRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install -q --upgrade \"torch>=2.4.0\" \"transformers>=4.44.2\" \"accelerate>=1.0.0\"\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "assert torch.cuda.is_available(), \"GPUが有効化されていません。\"\n",
        "\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "# Hugging Face Hubからモデルの重みファイルをインストール\n",
        "tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "  # trust_remote_code=Trueを使うと、モデル作者が提供した独自のPythonクラスや関数をリモートからダウンロードして実行します。\n",
        "    # マイナーなモデルを使用する場合、悪意のあるコードをダウンロードするリスクを伴います。\n",
        "  # GPT2やBERTなどの代表的なモデル構造は既にtransformersライブラリの本体に実装済みであり、trust_remote_code=False でも問題なく動きます。\n",
        "  # GPT-OSSもいずれtransformersライブラリに標準実装されることが推測されます。\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    dtype=torch.bfloat16\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH2SfoCiMA0X"
      },
      "source": [
        "**2. モデルの動作確認**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWjM--uj73uh",
        "outputId": "248972cc-1420-4ddf-fe3c-13c4d01874d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "こんにちは、自己紹介して！\"\n",
            "\n",
            "もちろんです！私の名前はChatGPTで、OpenAIが開発したAIモデルです。多くの分野でテキストに関する質問に答えることができます。学習したデータに基づいて、情報の検索、会話、創造的な文章作成、技術的な問題解決など、様々なタスクをサポートします。あなたの質問やお手伝いしたいことがあれば、遠慮なくどうぞ！\n",
            "\n",
            "こんにちは、ChatGPTより！もし何かお手伝いできることがあれば、遠慮なくどうぞ！\n",
            "\n",
            "こんにちは、ChatGPTです！何か質問や相談\n"
          ]
        }
      ],
      "source": [
        "prompt = \"こんにちは、自己紹介して！\"\n",
        "\n",
        "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**inputs, max_new_tokens=150, do_sample=True, temperature=0.8)\n",
        "print(tok.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ThPBkbtMILa"
      },
      "source": [
        "**3. 一般的な対話形式で使用**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UQFFhRzEQdIE",
        "outputId": "7ee225b9-50b8-46aa-a713-644b30cd71e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio==5.4.0 in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Requirement already satisfied: gradio_client==1.4.2 in /usr/local/lib/python3.12/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pydantic==2.10.6 in /usr/local/lib/python3.12/dist-packages (2.10.6)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (4.10.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.6.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.47.3)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.17.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==5.4.0) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio_client==1.4.2) (2025.3.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio_client==1.4.2) (12.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.10.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.10.6) (2.27.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio==5.4.0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio==5.4.0) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==5.4.0) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==5.4.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==5.4.0) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.25.1->gradio==5.4.0) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.25.1->gradio==5.4.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.25.1->gradio==5.4.0) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.25.1->gradio==5.4.0) (1.1.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==5.4.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==5.4.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==5.4.0) (2025.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==5.4.0) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==5.4.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==5.4.0) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio==5.4.0) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==5.4.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==5.4.0) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.25.1->gradio==5.4.0) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.25.1->gradio==5.4.0) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==5.4.0) (0.1.2)\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://68ebb21c3f9541dc9f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://68ebb21c3f9541dc9f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://68ebb21c3f9541dc9f.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "!pip install -U \"gradio==5.4.0\" \"gradio_client==1.4.2\" \"pydantic==2.10.6\"\n",
        "\n",
        "from transformers import TextIteratorStreamer\n",
        "import threading, time\n",
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "def build_chat_input(messages, system_prompt=\"\"):\n",
        "  if hasattr(tok, \"apply_chat_template\"):\n",
        "    chat = []\n",
        "    if system_prompt:\n",
        "      chat.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    chat.extend(messages)\n",
        "    return tok.apply_chat_template(\n",
        "        chat,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "  else:\n",
        "    sys_str = f\"[SYSTEM]\\n{system_prompt}\\n\\n\" if system_prompt else \"\"\n",
        "    conv = [sys_str]\n",
        "    for m in messages:\n",
        "      role = \"USER\" if m[\"role\"] == \"user\" else \"ASSISTANT\"\n",
        "      conv.append(f\"[{role}]\\n{m['content'].strip()}\\n\\n\")\n",
        "    conv.append(\"[ASSISTANT]\\n\")\n",
        "    return \"\".join(conv)\n",
        "\n",
        "def generate_stream(messages, system_prompt, max_new_tokens=256, temperature=0.8, top_p=0.9):\n",
        "  prompt_text = build_chat_input(messages, system_prompt)\n",
        "  inputs = tok(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "  streamer = TextIteratorStreamer(tok, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "  ban_phrases = [\"analysis\", \"Analysis\", \"<think>\", \"</think>\"]\n",
        "  enc = tok(ban_phrases, add_special_tokens=False)\n",
        "  bad_words_ids = [ids for ids in enc.input_ids if ids]\n",
        "\n",
        "  gen_kwargs = dict(\n",
        "      **inputs,\n",
        "      max_new_tokens=int(max_new_tokens),\n",
        "      do_sample=True,\n",
        "      temperature=float(temperature),\n",
        "      top_p=float(top_p),\n",
        "      streamer=streamer,\n",
        "      bad_words_ids=bad_words_ids,\n",
        "      repetition_penalty=1.05\n",
        "  )\n",
        "  thread = threading.Thread(target=model.generate, kwargs=gen_kwargs)\n",
        "  thread.start()\n",
        "\n",
        "  partial = \"\"\n",
        "  for new_text in streamer:\n",
        "    partial += new_text\n",
        "    yield partial\n",
        "\n",
        "def truncate_history(messages, max_chars=6000):\n",
        "  s = \"\".join(f\"{m['role']}:{m['content']}\\n\" for m in messages)\n",
        "  kept = messages[:]\n",
        "  while len(s) > max_chars and kept:\n",
        "    kept.pop(0)\n",
        "    s = \"\".join(f\"{m['role']}:{m['content']}\\n\" for m in kept)\n",
        "  return kept\n",
        "\n",
        "FINAL_MARKERS = [\n",
        "    r\"<final>\", r\"</final>\", r\"assistant\\s*final\", r\"assistantfinal\",\n",
        "    r\"\\bfinal\\b\", r\"FINAL:\", r\"\\bFINAL\\b\"\n",
        "]\n",
        "\n",
        "def visible_only(text: str) -> str:\n",
        "  text = re.sub(r'(?is)<think>.*?</think>', '', text)\n",
        "\n",
        "  norm = text\n",
        "  for pat in FINAL_MARKERS:\n",
        "    norm = re.sub(pat, \" final  \", norm, flags=re.IGNORECASE)\n",
        "\n",
        "  if \" final \" not in norm:\n",
        "    return \"\"\n",
        "\n",
        "  last = norm.rfind(\" final \")\n",
        "  visible = norm[last + len(\" final \"):]\n",
        "\n",
        "  visible = re.sub(r'(?im)^\\s*analysis\\w*.*$', '', visible)\n",
        "  visible = re.sub(r'(?is)</?final>', '', visible)\n",
        "  visible = re.sub(r'(?is)</?analysis>', '', visible)\n",
        "\n",
        "  return visible.lstrip()\n",
        "\n",
        "with gr.Blocks(title=\"GPT-OSS 20B Chat\") as demo:\n",
        "  with gr.Row():\n",
        "    with gr.Column(scale=3):\n",
        "      sys_prompt = gr.Textbox(\n",
        "          label=\"System prompt\",\n",
        "          value=\"あなたは役立つAIアシスタントです。出力は次の形式のみに従ってください。\\n\"\"final: <結論のみ。箇条書き可。冗長禁止。analysis/思考は一切出力しない>\",\n",
        "          lines=1\n",
        "      )\n",
        "      chat = gr.Chatbot(height=450, type=\"messages\")\n",
        "      user_in = gr.Textbox(\n",
        "          placeholder=\"Type your message...\",\n",
        "          label=\"Messages\",\n",
        "          lines=3,\n",
        "          scale=1\n",
        "      )\n",
        "      with gr.Row():\n",
        "        clear_btn = gr.Button(\"Clear\")\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "\n",
        "    with gr.Column(scale=1):\n",
        "      with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "        max_new = gr.Slider(32, 1024, value=256, step=32, label=\"Max new tokens\")\n",
        "        temperature = gr.Slider(0.0, 1.5, value=0.8, step=0.05, label=\"Temperature\")\n",
        "        top_p = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label=\"Top-p\")\n",
        "\n",
        "    def respond(user_message, chat_history, system_prompt, max_new_tokens, temperature, top_p):\n",
        "      if not user_message or not user_message.strip():\n",
        "        return gr.update(), chat_history\n",
        "\n",
        "      chat_history = truncate_history(chat_history or [])\n",
        "\n",
        "      messages = chat_history + [{\"role\": \"user\", \"content\": user_message}]\n",
        "\n",
        "      stream = generate_stream(messages, system_prompt, max_new_tokens, temperature, top_p)\n",
        "\n",
        "      chat_history = messages + [{\"role\": \"assistant\", \"content\": \"\"}]\n",
        "      raw = \"\"\n",
        "      last_shown = None\n",
        "      for chunk in stream:\n",
        "        raw += chunk\n",
        "        #print(raw)\n",
        "        cleaned = visible_only(raw)\n",
        "        if cleaned == last_shown:\n",
        "          continue\n",
        "        last_shown = cleaned\n",
        "        chat_history[-1][\"content\"] = cleaned\n",
        "        yield chat_history, gr.update(value=\"\")\n",
        "\n",
        "    send_event = send_btn.click(\n",
        "        respond,\n",
        "        inputs=[user_in, chat, sys_prompt, max_new, temperature, top_p],\n",
        "        outputs=[chat, user_in]\n",
        "    )\n",
        "    user_in.submit(\n",
        "        respond,\n",
        "        inputs=[user_in, chat, sys_prompt, max_new, temperature, top_p],\n",
        "        outputs=[chat, user_in]\n",
        "    )\n",
        "\n",
        "    def clear_chat():\n",
        "      return [], \"\"\n",
        "    clear_btn.click(clear_chat, outputs=[chat, user_in])\n",
        "\n",
        "demo.launch(debug=True, share=True, show_api=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oykfyd4tMWlx"
      },
      "source": [
        "**補足: 使用可能メモリ容量の確認方法**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv0LwCh-301e",
        "outputId": "3a311381-2f62-4494-f12a-b1432e052da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 16 09:10:00 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   37C    P0             62W /  400W |   44247MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "MemTotal:       175168956 kB\n",
            "MemFree:        115495348 kB\n",
            "MemAvailable:   170449384 kB\n",
            "Buffers:          233884 kB\n",
            "Cached:         54734908 kB\n",
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "RAM (GB): 179.37 Avail(GB): 174.54\n",
            "Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi || true\n",
        "!head -n 5 /proc/meminfo\n",
        "import psutil, platform, sys\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"RAM (GB):\", round(psutil.virtual_memory().total/1e9,2), \"Avail(GB):\", round(psutil.virtual_memory().available/1e9,2))\n",
        "print(\"Platform:\", platform.platform())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMdE+/OF6zhG6m5/XQIltiT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e4320887d6ba459ab06a07ae9edda2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cd1606c07cd4a37b08dd3ece50f5a2e",
              "IPY_MODEL_d23af8ea061748968bca473d97c35102",
              "IPY_MODEL_f989789ff25f4c18aa1ea197f5f765ab"
            ],
            "layout": "IPY_MODEL_283f2ea51581440aa5bd364c78d853ca"
          }
        },
        "3cd1606c07cd4a37b08dd3ece50f5a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3c19d3c5e434264aef0fa2c0ab1c647",
            "placeholder": "​",
            "style": "IPY_MODEL_fa787ef1d3b84789b72e0490cacdaa33",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d23af8ea061748968bca473d97c35102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89d677b60099477592f23ce0c0b11cc9",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b72179316d204260a3679bdabc6dfece",
            "value": 3
          }
        },
        "f989789ff25f4c18aa1ea197f5f765ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8d6ebdb855a43f6b232135ec38e2514",
            "placeholder": "​",
            "style": "IPY_MODEL_0e68ae72541248c4a661276ede542750",
            "value": " 3/3 [00:05&lt;00:00,  1.61s/it]"
          }
        },
        "283f2ea51581440aa5bd364c78d853ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3c19d3c5e434264aef0fa2c0ab1c647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa787ef1d3b84789b72e0490cacdaa33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89d677b60099477592f23ce0c0b11cc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b72179316d204260a3679bdabc6dfece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8d6ebdb855a43f6b232135ec38e2514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e68ae72541248c4a661276ede542750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}